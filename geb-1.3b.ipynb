{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21682f1d-6e31-4bbc-a66d-1f32dbf3574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evals\n",
    "import torch\n",
    "import logging\n",
    "import transformers\n",
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from peft import LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86f2623-918a-4204-a1a0-0df61d83bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize logger.\n",
    "\"\"\"\n",
    "transformers.utils.logging.set_verbosity_info()\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ee540a-f626-42a6-8e00-e20cf99f4146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1247d6f4dc14dae91485247cc65549d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/javen/.cache/huggingface/hub/models--GEB-AGI--geb-1.3b/snapshots/3612b088ae491042c6f7d590e6313463b11f00c9/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b127f8cd53453499f653a5d0b09bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_geblm.py:   0%|          | 0.00/2.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/GEB-AGI/geb-1.3b:\n",
      "- configuration_geblm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "loading configuration file config.json from cache at /home/javen/.cache/huggingface/hub/models--GEB-AGI--geb-1.3b/snapshots/3612b088ae491042c6f7d590e6313463b11f00c9/config.json\n",
      "Model config GEBConfig {\n",
      "  \"_name_or_path\": \"GEB-AGI/geb-1.3b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"apply_query_key_layer_scaling\": false,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"GEBForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": false,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"GEB-AGI/geb-1.3b--configuration_geblm.GEBConfig\",\n",
      "    \"AutoModel\": \"GEB-AGI/geb-1.3b--modeling_geb.GEBForCausalLM\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 5632,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": [\n",
      "    0.0\n",
      "  ],\n",
      "  \"hidden_size\": 2048,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"geblm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"num_layers\": 24,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"padded_vocab_size\": 64896,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"repetition_penalty\": 1.15,\n",
      "  \"seq_length\": 4096,\n",
      "  \"temperature\": 0.3,\n",
      "  \"top_k\": 5,\n",
      "  \"top_p\": 0.5,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_bfloat16\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"use_flash_attn\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60cde463cd74cfa8fc1608315ec668a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_geb.py:   0%|          | 0.00/50.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-25 17:48:51,071] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2024-10-25 17:48:51,072] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/GEB-AGI/geb-1.3b:\n",
      "- modeling_geb.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5e6e96c8f846ce9c6698b9d7b54a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /home/javen/.cache/huggingface/hub/models--GEB-AGI--geb-1.3b/snapshots/3612b088ae491042c6f7d590e6313463b11f00c9/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 512,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"repetition_penalty\": 1.15,\n",
      "  \"temperature\": 0.3,\n",
      "  \"top_k\": 5,\n",
      "  \"top_p\": 0.5\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GEBForCausalLM.\n",
      "\n",
      "All the weights of GEBForCausalLM were initialized from the model checkpoint at GEB-AGI/geb-1.3b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GEBForCausalLM for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3973256ee4f1426cbfd9e69b1b7d7671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557b4bff562c467cb54ee9afacee1f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_geb.py:   0%|          | 0.00/10.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/GEB-AGI/geb-1.3b:\n",
      "- tokenization_geb.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97481bb9ef848f89c7bef8cf85abdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GEBtokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file GEBtokenizer.model from cache at /home/javen/.cache/huggingface/hub/models--GEB-AGI--geb-1.3b/snapshots/3612b088ae491042c6f7d590e6313463b11f00c9/GEBtokenizer.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/javen/.cache/huggingface/hub/models--GEB-AGI--geb-1.3b/snapshots/3612b088ae491042c6f7d590e6313463b11f00c9/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 1, '<eos>': 2, '<pad>': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load model and tokenizer.\n",
    "\"\"\"\n",
    "# set seed before initializing model\n",
    "set_seed(777)\n",
    "\n",
    "# modified model & tokenizer code (due to transformers compatibility issue)\n",
    "# model_id = \"/home/javen/Projects/geb-1.3b\"\n",
    "model_id = \"GEB-AGI/geb-1.3b\"\n",
    "\n",
    "# load model\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True).bfloat16() #.cuda()\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, trust_remote_code=True).bfloat16()\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).bfloat16()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "print(tokenizer.special_tokens)\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c4c0ee3-d23e-4e96-a3d6-d88b26c9b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215365\n",
      "100\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['headline', 'title', 'text', '__index_level_0__'],\n",
      "        num_rows: 85\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['headline', 'title', 'text', '__index_level_0__'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nlabel_pad_token_id = tokenizer.pad_token_id\\ndata_collator = DataCollatorForSeq2Seq(\\n    tokenizer,\\n    model=model,\\n    label_pad_token_id=label_pad_token_id,\\n    # pad_to_multiple_of=8 if training_args.fp16 else None,\\n    pad_to_multiple_of=8,\\n)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load & prepare WikiHow dataset.\n",
    "\n",
    "https://huggingface.co/datasets/gursi26/wikihow-cleaned\n",
    "https://github.com/mahnazkoupaee/WikiHow-Dataset\n",
    "\"\"\"\n",
    "def clean_dataset(dataset):\n",
    "    df = pd.DataFrame(dataset)\n",
    "    print(len(df))\n",
    "    df = df.dropna()\n",
    "    df = df.iloc[:100]\n",
    "    print(len(df))\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "def load_dataset():\n",
    "    df = pd.read_csv(\"/home/javen/Projects/wikihowAll.csv\")\n",
    "    # dataset = load_dataset(\"gursi26/wikihow-cleaned\", split=\"train\")\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset\n",
    "\n",
    "# load dataset\n",
    "dataset = load_dataset()\n",
    "dataset = clean_dataset(dataset)\n",
    "dataset = dataset.train_test_split(test_size=0.15)\n",
    "print(dataset)\n",
    "\n",
    "# data collator\n",
    "\"\"\"\n",
    "label_pad_token_id = tokenizer.pad_token_id\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    # pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe575e1f-fdba-4449-a85c-7b8442370283",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation metrics.\n",
    "\"\"\"\n",
    "def compute_metrics(eval_preds):\n",
    "    # prepare prediction data\n",
    "    labels, preds = eval_preds.label_ids, eval_preds.predictions\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # decode\n",
    "    preds_decoded = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels_decoded = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # calculate rouge scores\n",
    "    rouge_scores = eval.calculate_rouge(labels_decoded, preds_decoded)\n",
    "\n",
    "    # return metrics\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddfd2308-b1a0-40f1-8fcf-ebb0bcd166f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 1\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c978f4773e954b6fb60b293a0c59407c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca605044be434a62b19edd5a4269bbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create Trainer.\n",
    "\"\"\"\n",
    "\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_r = 16\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    output_dir='./output',\n",
    "    learning_rate=5e-05,\n",
    "    logging_steps=1,\n",
    "    logging_dir='./logs',\n",
    "    log_level='debug',\n",
    "    save_steps=5,\n",
    "    use_cpu=True,\n",
    "    label_names=['summary'],\n",
    "    max_steps=2,\n",
    "    # num_train_epochs=3,\n",
    "    # eval_strategy='epoch',\n",
    "    eval_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # optim=\"paged_adamw_32bit\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e03e77-8331-466d-9233-8e20db833a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train model.\n",
    "\"\"\"\n",
    "training_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f85fd5b6-7e2e-47c9-8762-37f4661b322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate summaries\n",
    "\"\"\"\n",
    "def summarize(model, text: str):\n",
    "    # encode input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs_length = len(inputs[\"input_ids\"][0])\n",
    "    print(inputs_length)\n",
    "\n",
    "    # generate new text with model\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        # print(outputs[0][inputs_length:])\n",
    "        print(len(outputs[0]))\n",
    "\n",
    "    # decode generated output text\n",
    "    decoded = tokenizer.decode(\n",
    "        # outputs[0][inputs_length:],\n",
    "        outputs[0][inputs_length:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return decoded\n",
    "\n",
    "def generate_summaries(model, dataset, tokenizer, num_samples=5):\n",
    "    summaries = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        print(i)\n",
    "        prompt = example['text']\n",
    "        summary = summarize(model, prompt)\n",
    "        summaries.append({'text': prompt, 'summary': summary})\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8cad4-efdd-451b-a073-3c269de7f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate model.\n",
    "\"\"\"\n",
    "summaries_hat = generate_summaries(model, dataset['test'], tokenizer)\n",
    "print(summaries_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63ddc7-ad66-493f-96ad-254047b7fdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
