{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21682f1d-6e31-4bbc-a66d-1f32dbf3574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/javen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/javen/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evals\n",
    "import torch\n",
    "import logging\n",
    "import transformers\n",
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from peft import LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86f2623-918a-4204-a1a0-0df61d83bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize logger.\n",
    "\"\"\"\n",
    "transformers.utils.logging.set_verbosity_info()\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ee540a-f626-42a6-8e00-e20cf99f4146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/javen/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"sshleifer/distilbart-xsum-12-3\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 3,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"eos_token_ids\": [\n",
      "    2\n",
      "  ],\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 62,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 11,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 6,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"save_step\": 58,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {},\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/javen/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-xsum-12-3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading configuration file config.json from cache at /home/javen/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"sshleifer/distilbart-xsum-12-3\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 3,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"eos_token_ids\": [\n",
      "    2\n",
      "  ],\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 62,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 11,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 6,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"save_step\": 58,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {},\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/javen/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/vocab.json\n",
      "loading file merges.txt from cache at /home/javen/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/javen/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/javen/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/javen/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"sshleifer/distilbart-xsum-12-3\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 3,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"eos_token_ids\": [\n",
      "    2\n",
      "  ],\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 62,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 11,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 6,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"save_step\": 58,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {},\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/javen/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"sshleifer/distilbart-xsum-12-3\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 3,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"eos_token_ids\": [\n",
      "    2\n",
      "  ],\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 62,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 11,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 6,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"save_step\": 58,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {},\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load model and tokenizer.\n",
    "\"\"\"\n",
    "# set seed before initializing model\n",
    "set_seed(777)\n",
    "\n",
    "# modified model & tokenizer code (due to transformers compatibility issue)\n",
    "# model_id = \"/home/javen/Projects/geb-1.3b\"\n",
    "# model_id = \"GEB-AGI/geb-1.3b\"\n",
    "# model_id = \"t5-small\"\n",
    "model_id = \"sshleifer/distilbart-xsum-12-3\"\n",
    "\n",
    "# load model\n",
    "# model = AutoModel.from_pretrained(model_id, trust_remote_code=True).bfloat16() #.cuda()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, trust_remote_code=True).bfloat16()\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).bfloat16()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "# print(tokenizer.special_tokens)\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c4c0ee3-d23e-4e96-a3d6-d88b26c9b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214293\n",
      "213892\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['summary', 'title', 'text', '__index_level_0__'],\n",
      "        num_rows: 160419\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['summary', 'title', 'text', '__index_level_0__'],\n",
      "        num_rows: 26737\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['summary', 'title', 'text', '__index_level_0__'],\n",
      "        num_rows: 26736\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load & clean WikiHow dataset.\n",
    "\n",
    "https://huggingface.co/datasets/gursi26/wikihow-cleaned\n",
    "https://github.com/mahnazkoupaee/WikiHow-Dataset\n",
    "\"\"\"\n",
    "def clean_dataset(dataset):\n",
    "    df = pd.DataFrame(dataset)\n",
    "    print(len(df))\n",
    "    df = df.dropna()\n",
    "    # df = df.iloc[:100]\n",
    "    print(len(df))\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "def load_dataset():\n",
    "    df = pd.read_csv(\"/home/javen/Projects/wikihow-cleaned/wikihow-cleaned.csv\")\n",
    "    # dataset = load_dataset(\"gursi26/wikihow-cleaned\", split=\"train\")\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset\n",
    "\n",
    "# load dataset\n",
    "dataset = load_dataset()\n",
    "dataset = clean_dataset(dataset)\n",
    "\n",
    "# split dataset\n",
    "a = dataset.train_test_split(test_size=0.25)\n",
    "b = a['test'].train_test_split(test_size=0.5)\n",
    "dataset = DatasetDict({\n",
    "    'train': a['train'],\n",
    "    'test': b['test'],\n",
    "    'valid': b['train']\n",
    "})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d83db1-df34-42ae-8792-4a68d0fe8f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26736 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data tokenization & collation.\n",
    "\"\"\"\n",
    "\n",
    "# data collator\n",
    "label_pad_token_id = tokenizer.pad_token_id\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "max_input_length = 256\n",
    "max_target_length = 128\n",
    "\n",
    "if model_id in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples['text']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples['summary'], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# apply tokenization\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe575e1f-fdba-4449-a85c-7b8442370283",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation metrics.\n",
    "\"\"\"\n",
    "def compute_metrics(eval_preds):\n",
    "    # prepare prediction data\n",
    "    labels, preds = eval_preds.label_ids, eval_preds.predictions\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # decode\n",
    "    preds_decoded = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels_decoded = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # calculate rouge scores\n",
    "    rouge_scores = evals.calculate_rouge(labels_decoded, preds_decoded)\n",
    "\n",
    "    # return metrics\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddfd2308-b1a0-40f1-8fcf-ebb0bcd166f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create Trainer.\n",
    "\"\"\"\n",
    "batch_size = 16\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    learning_rate=5e-05,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    log_level='debug',\n",
    "    output_dir='./output',\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    use_cpu=True,\n",
    "    eval_strategy='epoch',\n",
    "    # eval_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    # greater_is_better=False,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['valid'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e03e77-8331-466d-9233-8e20db833a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 16\n",
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, title, __index_level_0__, text. If summary, title, __index_level_0__, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-26 12:04:41,959] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2024-10-26 12:04:41,961] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 160,419\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10,027\n",
      "  Number of trainable parameters = 255,120,384\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10027' max='10027' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10027/10027 263:57:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.007200</td>\n",
       "      <td>1.937126</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.151899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-50\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-50/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-50/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-50/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-50/special_tokens_map.json\n",
      "Saving model checkpoint to ./output/checkpoint-100\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-100/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-100/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-100/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-100/special_tokens_map.json\n",
      "Saving model checkpoint to ./output/checkpoint-150\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-150/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-150/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-150/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-150/special_tokens_map.json\n",
      "Saving model checkpoint to ./output/checkpoint-200\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-200/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-200/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-200/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-50] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-250\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-250/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-250/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-250/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-300\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-300/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-300/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-300/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-350\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-350/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-350/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-350/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-400\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-400/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-400/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-400/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-450\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-450/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-450/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-450/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-500\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-500/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-500/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-500/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-550\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-550/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-550/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-550/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-600\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-600/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-600/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-600/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-650\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-650/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-650/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-650/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-700\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-700/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-700/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-700/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-550] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-750\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-750/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-750/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-750/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-600] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-800\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-800/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-800/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-800/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-650] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-850\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-850/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-850/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-850/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-900\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-900/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-900/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-900/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-950\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-950/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-950/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-950/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1000\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1000/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1000/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1000/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-850] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1050\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1050/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1050/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1050/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1050/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1050/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-900] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1100\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1100/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1100/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1100/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-950] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1150\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1150/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1150/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1150/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1200\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1200/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1200/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1200/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1050] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1250\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1250/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1250/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1250/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1250/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1300\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1300/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1300/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1300/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1300/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1350\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1350/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1350/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1350/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1400\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1400/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1400/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1400/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1450\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1450/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1450/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1450/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1500\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1500/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1500/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1500/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1550\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1550/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1550/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1550/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1600\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1600/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1600/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1600/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1650\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1650/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1650/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1650/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1700\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1700/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1700/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1700/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1550] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1750\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1750/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1750/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1750/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1600] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1800\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1800/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1800/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1800/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1650] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1850\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1850/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1850/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1850/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1900\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1900/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1900/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1900/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-1950\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-1950/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-1950/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-1950/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-1950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2000\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2000/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2000/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2000/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1850] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2050\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2050/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2050/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2050/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2050/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2050/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1900] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2100\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2100/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2100/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2100/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-1950] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2150\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2150/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2150/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2150/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2200\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2200/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2200/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2200/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2050] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2250\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2250/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2250/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2250/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2250/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2300\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2300/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2300/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2300/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2300/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2350\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2350/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2350/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2350/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2400\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2400/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2400/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2400/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2450\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2450/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2450/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2450/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2500\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2500/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2500/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2500/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2550\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2550/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2550/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2550/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2600\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2600/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2600/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2600/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2650\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2650/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2650/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2650/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2700\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2700/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2700/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2700/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2550] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2750\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2750/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2750/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2750/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2600] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2800\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2800/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2800/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2800/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2650] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2850\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2850/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2850/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2850/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2900\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2900/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2900/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2900/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-2950\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-2950/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-2950/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-2950/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-2950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3000\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3000/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3000/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3000/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2850] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3050\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3050/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3050/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3050/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3050/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3050/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2900] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3100\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3100/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3100/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3100/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-2950] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3150\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3150/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3150/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3150/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3200\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3200/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3200/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3200/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3050] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3250\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3250/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3250/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3250/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3250/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3300\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3300/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3300/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3300/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3300/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3350\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3350/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3350/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3350/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3400\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3400/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3400/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3400/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3450\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3450/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3450/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3450/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3500\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3500/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3500/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3500/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3550\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3550/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3550/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3550/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3600\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3600/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3600/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3600/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3650\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3650/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3650/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3650/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3700\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3700/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3700/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3700/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3550] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3750\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3750/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3750/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3750/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3600] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3800\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3800/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3800/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3800/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3650] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3850\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3850/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3850/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3850/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3900\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3900/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3900/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3900/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-3950\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-3950/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-3950/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-3950/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-3950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4000\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4000/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4000/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4000/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3850] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4050\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4050/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4050/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4050/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4050/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4050/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3900] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4100\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4100/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4100/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4100/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-3950] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4150\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4150/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4150/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4150/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4200\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4200/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4200/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4200/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4050] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4250\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4250/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4250/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4250/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4250/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4300\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4300/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4300/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4300/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4300/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4350\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4350/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4350/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4350/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4400\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4400/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4400/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4400/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4450\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4450/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4450/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4450/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4500\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4500/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4500/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4500/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4550\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4550/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4550/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4550/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4600\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4600/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4600/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4600/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4650\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4650/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4650/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4650/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4700\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4700/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4700/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4700/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4550] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4750\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4750/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4750/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4750/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4600] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4800\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4800/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4800/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4800/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4650] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4850\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4850/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4850/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4850/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4900\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4900/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4900/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4900/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-4950\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-4950/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-4950/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-4950/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-4950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5000\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5000/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5000/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5000/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4850] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5050\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5050/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5050/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5050/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5050/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5050/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4900] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5100\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5100/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5100/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5100/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-4950] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5150\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5150/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5150/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5150/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5200\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5200/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5200/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5200/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5050] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5250\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5250/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5250/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5250/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5250/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5300\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5300/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5300/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5300/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5300/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5350\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5350/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5350/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5350/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5400\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5400/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5400/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5400/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5450\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5450/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5450/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5450/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5500\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5500/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5500/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5500/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5550\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5550/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5550/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5550/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5600\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5600/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5600/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5600/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5650\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5650/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5650/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5650/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5700\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5700/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5700/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5700/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5550] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5750\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5750/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5750/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5750/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5600] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5800\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5800/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5800/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5800/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5650] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5850\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5850/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5850/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5850/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5900\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5900/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5900/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5900/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-5950\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-5950/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-5950/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-5950/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-5950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6000\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6000/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6000/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6000/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5850] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6050\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6050/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6050/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6050/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6050/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6050/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5900] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6100\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6100/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6100/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6100/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-5950] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6150\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6150/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6150/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6150/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6200\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6200/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6200/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6200/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6050] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6250\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6250/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6250/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6250/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6250/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6300\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6300/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6300/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6300/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6300/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6350\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6350/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6350/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6350/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6400\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6400/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6400/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6400/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6450\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6450/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6450/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6450/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6500\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6500/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6500/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6500/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6550\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6550/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6550/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6550/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6600\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6600/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6600/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6600/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6650\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6650/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6650/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6650/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6700\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6700/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6700/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6700/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6550] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6750\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6750/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6750/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6750/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6600] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6800\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6800/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6800/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6800/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6650] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6850\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6850/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6850/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6850/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6900\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6900/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6900/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6900/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-6950\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-6950/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-6950/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-6950/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-6950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7000\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7000/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7000/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7000/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6850] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7050\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7050/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7050/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7050/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7050/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7050/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6900] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7100\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7100/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7100/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7100/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-6950] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7150\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7150/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7150/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7150/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7200\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7200/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7200/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7200/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7050] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7250\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7250/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7250/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7250/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7250/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7300\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7300/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7300/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7300/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7300/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7350\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7350/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7350/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7350/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7400\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7400/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7400/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7400/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7450\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7450/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7450/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7450/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7500\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7500/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7500/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7500/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7550\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7550/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7550/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7550/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7600\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7600/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7600/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7600/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7650\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7650/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7650/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7650/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7700\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7700/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7700/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7700/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7550] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7750\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7750/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7750/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7750/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7600] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7800\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7800/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7800/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7800/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7650] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7850\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7850/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7850/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7850/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7900\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7900/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7900/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7900/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-7950\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-7950/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-7950/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-7950/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-7950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8000\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8000/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8000/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8000/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7850] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8050\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8050/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8050/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8050/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8050/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8050/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7900] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8100\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8100/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8100/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8100/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-7950] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8150\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8150/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8150/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8150/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8200\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8200/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8200/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8200/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8050] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8250\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8250/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8250/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8250/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8250/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8300\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8300/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8300/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8300/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8300/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8350\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8350/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8350/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8350/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8400\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8400/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8400/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8400/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8450\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8450/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8450/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8450/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8500\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8500/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8500/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8500/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8550\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8550/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8550/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8550/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8600\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8600/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8600/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8600/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8650\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8650/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8650/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8650/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8700\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8700/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8700/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8700/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8550] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8750\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8750/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8750/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8750/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8600] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8800\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8800/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8800/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8800/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8650] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8850\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8850/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8850/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8850/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8900\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8900/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8900/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8900/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-8950\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-8950/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-8950/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-8950/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-8950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9000\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9000/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9000/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9000/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8850] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9050\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9050/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9050/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9050/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9050/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9050/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8900] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9100\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9100/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9100/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9100/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-8950] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9150\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9150/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9150/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9150/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9200\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9200/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9200/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9200/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9050] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9250\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9250/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9250/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9250/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9250/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9300\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9300/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9300/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9300/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9300/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9350\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9350/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9350/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9350/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9400\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9400/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9400/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9400/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9450\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9450/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9450/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9450/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9500\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9500/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9500/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9500/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9550\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9550/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9550/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9550/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9600\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9600/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9600/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9600/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9650\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9650/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9650/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9650/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9700\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9700/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9700/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9700/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9550] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9750\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9750/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9750/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9750/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9600] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9800\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9800/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9800/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9800/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9650] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9850\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9850/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9850/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9850/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9900\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9900/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9900/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9900/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-9950\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-9950/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-9950/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-9950/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-9950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/checkpoint-10000\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Configuration saved in ./output/checkpoint-10000/config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Configuration saved in ./output/checkpoint-10000/generation_config.json\n",
      "Model weights saved in ./output/checkpoint-10000/model.safetensors\n",
      "tokenizer config file saved in ./output/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/checkpoint-9850] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, title, __index_level_0__, text. If summary, title, __index_level_0__, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26736\n",
      "  Batch size = 16\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 62,\n",
      "  \"min_length\": 11,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 6,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train model.\n",
    "\"\"\"\n",
    "training_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f85fd5b6-7e2e-47c9-8762-37f4661b322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate summaries\n",
    "\"\"\"\n",
    "def summarize(model, text: str):\n",
    "    # encode input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs_length = len(inputs[\"input_ids\"][0])\n",
    "    print(inputs_length)\n",
    "\n",
    "    # generate new text with model\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        # print(outputs[0][inputs_length:])\n",
    "        print(len(outputs[0]))\n",
    "\n",
    "    # decode generated output text\n",
    "    decoded = tokenizer.decode(\n",
    "        # outputs[0][inputs_length:],\n",
    "        outputs[0][inputs_length:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return decoded\n",
    "\n",
    "def generate_summaries(model, dataset, tokenizer, num_samples=5):\n",
    "    summaries = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        print(i)\n",
    "        prompt = example['text']\n",
    "        summary = summarize(model, prompt)\n",
    "        summaries.append({'text': prompt, 'summary': summary})\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec8cad4-efdd-451b-a073-3c269de7f505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEvaluate model.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluate model.\n",
    "\"\"\"\n",
    "# summaries_hat = generate_summaries(model, dataset['test'], tokenizer)\n",
    "# print(summaries_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63ddc7-ad66-493f-96ad-254047b7fdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
